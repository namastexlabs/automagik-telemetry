> [!WARNING]
> **This document has been deprecated and moved to the archive.**
> 
> ðŸ“ **New Location:** See [docs/INDEX.md](../INDEX.md) for updated documentation.
> 
> **Replacements:**
> - Configuration: [docs/USER_GUIDES/CONFIGURATION.md](../USER_GUIDES/CONFIGURATION.md)
> - Quick Start: [docs/GETTING_STARTED.md](../GETTING_STARTED.md)
> - Backends: [docs/USER_GUIDES/BACKENDS.md](../USER_GUIDES/BACKENDS.md)
> - Testing: [docs/DEVELOPER_GUIDES/TESTING.md](../DEVELOPER_GUIDES/TESTING.md)
> - Architecture: [docs/DEVELOPER_GUIDES/ARCHITECTURE.md](../DEVELOPER_GUIDES/ARCHITECTURE.md)
> - Privacy: [docs/USER_GUIDES/PRIVACY.md](../USER_GUIDES/PRIVACY.md)
> 
> This file is kept for historical reference only.

---

[Original content below...]

# ClickHouse Backend Migration and Usage Guide

> **Direct ClickHouse integration for high-performance, self-hosted telemetry**

This guide covers everything you need to know about using the ClickHouse backend in Automagik Telemetry - from initial setup to production deployment.

---

## Table of Contents

1. [Overview](#1-overview)
2. [Getting Started](#2-getting-started)
3. [Configuration](#3-configuration)
4. [Migration from OTLP](#4-migration-from-otlp)
5. [Advanced Usage](#5-advanced-usage)
6. [Troubleshooting](#6-troubleshooting)
7. [Production Deployment](#7-production-deployment)

---

## 1. Overview

### What is the ClickHouse Backend?

The ClickHouse backend is a **direct integration** that bypasses the OpenTelemetry Collector and writes telemetry data straight to ClickHouse's HTTP API. It's designed for:

- **Self-hosted environments** where you control your own ClickHouse instance
- **Local development** with the full telemetry stack running on your machine
- **High-performance scenarios** where you want minimal latency
- **Custom schemas** that don't fit the standard OTLP format

### Architecture Comparison

**Traditional OTLP Flow:**
```
Your App â†’ OTLP/HTTP â†’ Collector â†’ ClickHouse Exporter â†’ ClickHouse
          (network)     (process)    (transformation)    (storage)
```

**ClickHouse Backend Flow:**
```
Your App â†’ ClickHouse Backend â†’ ClickHouse HTTP API â†’ ClickHouse
          (in-process)         (network)              (storage)
```

### When to Use ClickHouse vs OTLP

| Use Case | Recommended Backend |
|----------|-------------------|
| **Production (SaaS)** | OTLP - Use our hosted endpoint |
| **Self-hosted** | ClickHouse - Direct, no middleware |
| **Local development** | ClickHouse - Faster, simpler stack |
| **Custom schema** | ClickHouse - Full control over data |
| **Standard OTLP compliance** | OTLP - Industry standard |
| **Multi-backend forwarding** | OTLP - Collector handles routing |

### Benefits and Trade-offs

**Benefits:**
- **Simpler architecture** - Fewer components, less to debug
- **Better performance** - Direct insertion, no middleware overhead
- **Full control** - Own your data transformation and schema
- **Zero external dependencies** - Uses only stdlib (urllib, http)
- **Easier debugging** - Direct logs, no collector black box

**Trade-offs:**
- **Vendor lock-in** - Tied to ClickHouse (vs. OTLP's flexibility)
- **No multi-backend** - Can't forward to multiple destinations
- **Schema ownership** - You manage schema evolution
- **Self-hosted only** - Requires your own ClickHouse instance

---

## 2. Getting Started

### Prerequisites

**Required:**
- ClickHouse server running and accessible
- Python 3.12+ OR Node.js 18+ (depending on SDK)
- Database and table created (see below)

**Optional but Recommended:**
- Docker and Docker Compose (for local setup)
- Grafana (for visualization)

### Quick Start with Docker Compose

The fastest way to get started is using our pre-configured Docker setup:

```bash
# Clone the repository
git clone https://github.com/namastexlabs/automagik-telemetry.git
cd automagik-telemetry/infra

# Start ClickHouse, Collector, and Grafana
make start

# Verify ClickHouse is running
make check
```

This starts:
- **ClickHouse** on `http://localhost:8123`
- **Grafana** on `http://localhost:3000` (admin/admin)
- **OTLP Collector** on `http://localhost:4318` (optional)

### Manual ClickHouse Setup

If you prefer manual setup:

**1. Start ClickHouse:**
```bash
# Using Docker
docker run -d \
  --name clickhouse \
  -p 8123:8123 \
  -p 9000:9000 \
  clickhouse/clickhouse-server:24-alpine

# Or install locally: https://clickhouse.com/docs/en/install
```

**2. Create database and table:**
```sql
-- Connect to ClickHouse
clickhouse-client --host localhost --port 9000

-- Create database
CREATE DATABASE IF NOT EXISTS telemetry;

-- Create traces table
CREATE TABLE IF NOT EXISTS telemetry.traces (
    trace_id String,
    span_id String,
    parent_span_id String,
    timestamp DateTime,
    timestamp_ns UInt64,
    duration_ms UInt32,
    service_name String,
    span_name String,
    span_kind String,
    status_code String,
    status_message String,
    project_name String,
    project_version String,
    environment String DEFAULT 'production',
    hostname String,
    attributes Map(String, String),
    user_id String,
    session_id String,
    os_type String,
    os_version String,
    runtime_name String,
    runtime_version String
) ENGINE = MergeTree()
PARTITION BY toYYYYMM(timestamp)
ORDER BY (project_name, timestamp, trace_id)
TTL timestamp + INTERVAL 30 DAY;
```

### Basic Setup - Python

**1. Install the SDK:**
```bash
pip install automagik-telemetry
```

**2. Use the ClickHouse backend:**
```python
from automagik_telemetry import AutomagikTelemetry
from automagik_telemetry.client import TelemetryConfig

# Configure for ClickHouse
config = TelemetryConfig(
    project_name="my-app",
    version="1.0.0",
    backend="clickhouse",  # Switch to ClickHouse
    clickhouse_endpoint="http://localhost:8123",
    clickhouse_database="telemetry",
    clickhouse_table="traces",
)

client = AutomagikTelemetry(config=config)

# Track events - they go directly to ClickHouse
client.track_event("user.login", {
    "user_id": "user-123",
    "method": "oauth"
})

# Ensure data is sent
client.flush()
```

### Basic Setup - TypeScript

**1. Install the SDK:**
```bash
npm install @automagik/telemetry
# or
pnpm add @automagik/telemetry
```

**2. Use the ClickHouse backend:**
```typescript
import { AutomagikTelemetry, TelemetryConfig } from '@automagik/telemetry';

// Configure for ClickHouse
const config: TelemetryConfig = {
    projectName: 'my-app',
    version: '1.0.0',
    backend: 'clickhouse',  // Switch to ClickHouse
    clickhouseEndpoint: 'http://localhost:8123',
    clickhouseDatabase: 'telemetry',
    clickhouseTable: 'traces',
};

const client = new AutomagikTelemetry(config);

// Track events - they go directly to ClickHouse
client.trackEvent('user.login', {
    userId: 'user-123',
    method: 'oauth'
});

// Ensure data is sent
await client.flush();
```

### Verification Steps

**1. Check ClickHouse received data:**
```bash
# Using clickhouse-client
clickhouse-client --query="SELECT count() FROM telemetry.traces"

# Using curl
curl "http://localhost:8123/?query=SELECT%20count()%20FROM%20telemetry.traces"
```

**2. View the data:**
```bash
clickhouse-client --query="
SELECT
    timestamp,
    project_name,
    span_name,
    attributes
FROM telemetry.traces
ORDER BY timestamp DESC
LIMIT 10
FORMAT PrettyCompact
"
```

**3. Verify in Grafana:**
```bash
# Open Grafana
open http://localhost:3000

# Login: admin / admin
# Navigate to "Explore" â†’ Select ClickHouse datasource
# Query: SELECT * FROM traces LIMIT 10
```

---

## 3. Configuration

### All Configuration Options

The ClickHouse backend supports extensive configuration:

#### Python Configuration

```python
from automagik_telemetry.backends.clickhouse import ClickHouseBackend

backend = ClickHouseBackend(
    # Connection settings
    endpoint="http://localhost:8123",      # ClickHouse HTTP endpoint
    database="telemetry",                  # Database name
    table="traces",                        # Table name

    # Authentication
    username="default",                    # ClickHouse username
    password="",                          # ClickHouse password

    # Performance tuning
    timeout=5,                            # HTTP timeout (seconds)
    batch_size=100,                       # Rows per batch
    compression_enabled=True,              # Enable gzip compression

    # Reliability
    max_retries=3,                        # Maximum retry attempts
)
```

#### TypeScript Configuration

```typescript
import { ClickHouseBackend, ClickHouseBackendConfig } from '@automagik/telemetry';

const config: ClickHouseBackendConfig = {
    // Connection settings
    endpoint: 'http://localhost:8123',    // ClickHouse HTTP endpoint
    database: 'telemetry',                // Database name
    table: 'traces',                      // Table name

    // Authentication
    username: 'default',                  // ClickHouse username
    password: '',                         // ClickHouse password

    // Performance tuning
    timeout: 5000,                        // HTTP timeout (milliseconds)
    batchSize: 100,                       // Rows per batch
    compressionEnabled: true,              // Enable gzip compression

    // Reliability
    maxRetries: 3,                        // Maximum retry attempts
};

const backend = new ClickHouseBackend(config);
```

### Environment Variables

Configure via environment variables instead of code:

```bash
# Backend selection
export AUTOMAGIK_TELEMETRY_BACKEND=clickhouse

# ClickHouse connection
export AUTOMAGIK_TELEMETRY_CLICKHOUSE_ENDPOINT=http://localhost:8123
export AUTOMAGIK_TELEMETRY_CLICKHOUSE_DATABASE=telemetry
export AUTOMAGIK_TELEMETRY_CLICKHOUSE_TABLE=traces

# Authentication
export AUTOMAGIK_TELEMETRY_CLICKHOUSE_USERNAME=telemetry
export AUTOMAGIK_TELEMETRY_CLICKHOUSE_PASSWORD=telemetry_password

# Performance tuning
export AUTOMAGIK_TELEMETRY_CLICKHOUSE_BATCH_SIZE=100
export AUTOMAGIK_TELEMETRY_CLICKHOUSE_TIMEOUT=5

# Enable for debugging
export AUTOMAGIK_TELEMETRY_VERBOSE=true
```

Then in your code:

```python
# Python - uses env vars automatically
from automagik_telemetry import AutomagikTelemetry

client = AutomagikTelemetry(
    project_name="my-app",
    version="1.0.0"
)
# Backend and ClickHouse config read from env vars
```

```typescript
// TypeScript - uses env vars automatically
import { AutomagikTelemetry } from '@automagik/telemetry';

const client = new AutomagikTelemetry({
    projectName: 'my-app',
    version: '1.0.0'
});
// Backend and ClickHouse config read from env vars
```

### Configuration Object Examples

#### Development Configuration

```python
# Python
config = TelemetryConfig(
    project_name="my-app",
    version="1.0.0-dev",
    backend="clickhouse",
    clickhouse_endpoint="http://localhost:8123",
    clickhouse_database="telemetry",
    clickhouse_table="traces",
    clickhouse_username="default",
    clickhouse_password="",
    batch_size=1,  # Immediate flush for testing
    timeout=10,    # Longer timeout for debugging
)
```

```typescript
// TypeScript
const config: TelemetryConfig = {
    projectName: 'my-app',
    version: '1.0.0-dev',
    backend: 'clickhouse',
    clickhouseEndpoint: 'http://localhost:8123',
    clickhouseDatabase: 'telemetry',
    clickhouseTable: 'traces',
    clickhouseUsername: 'default',
    clickhousePassword: '',
    batchSize: 1,  // Immediate flush for testing
    timeout: 10000, // Longer timeout for debugging
};
```

#### Production Configuration

```python
# Python
config = TelemetryConfig(
    project_name="my-app",
    version="1.2.3",
    backend="clickhouse",
    clickhouse_endpoint="https://clickhouse.example.com:8443",
    clickhouse_database="telemetry",
    clickhouse_table="traces",
    clickhouse_username="app_user",
    clickhouse_password=os.getenv("CLICKHOUSE_PASSWORD"),
    batch_size=500,    # Larger batches for efficiency
    timeout=5,         # Fail fast
    compression_enabled=True,
    max_retries=3,
)
```

```typescript
// TypeScript
const config: TelemetryConfig = {
    projectName: 'my-app',
    version: '1.2.3',
    backend: 'clickhouse',
    clickhouseEndpoint: 'https://clickhouse.example.com:8443',
    clickhouseDatabase: 'telemetry',
    clickhouseTable: 'traces',
    clickhouseUsername: 'app_user',
    clickhousePassword: process.env.CLICKHOUSE_PASSWORD,
    batchSize: 500,    // Larger batches for efficiency
    timeout: 5000,     // Fail fast
    compressionEnabled: true,
    maxRetries: 3,
};
```

### Common Configurations

#### High-Throughput Application

```python
# Optimized for high event volume
config = TelemetryConfig(
    backend="clickhouse",
    batch_size=1000,           # Large batches
    compression_enabled=True,   # Compress everything
    timeout=10,                # Allow time for large batches
)
```

#### Low-Latency Application

```python
# Optimized for immediate visibility
config = TelemetryConfig(
    backend="clickhouse",
    batch_size=10,             # Small batches
    compression_enabled=False,  # Skip compression overhead
    timeout=2,                 # Fail fast
)
```

#### Secure Production

```python
# Full security configuration
config = TelemetryConfig(
    backend="clickhouse",
    clickhouse_endpoint="https://clickhouse.internal:8443",  # HTTPS
    clickhouse_username="telemetry_writer",
    clickhouse_password=os.getenv("CLICKHOUSE_PASSWORD"),    # From secrets
    timeout=5,
    max_retries=3,
)
```

---

## 4. Migration from OTLP

### Why Migrate?

You might want to migrate from OTLP to ClickHouse if:

- You're self-hosting and want to simplify your stack
- You're experiencing issues with the OTLP Collector
- You want direct control over data transformation
- You need better performance for high-volume telemetry

### Migration Checklist

- [ ] ClickHouse server is running and accessible
- [ ] Database and table schema are created
- [ ] Authentication credentials are configured
- [ ] Code changes are ready (see below)
- [ ] Tests pass with ClickHouse backend
- [ ] Monitoring is updated (if needed)

### What Changes Are Needed

**Minimal changes required!** The API remains identical - only configuration changes.

#### Python Migration

**Before (OTLP):**
```python
from automagik_telemetry import AutomagikTelemetry

client = AutomagikTelemetry(
    project_name="my-app",
    version="1.0.0",
    endpoint="https://telemetry.namastex.ai/v1/traces"  # OTLP endpoint
)
```

**After (ClickHouse):**
```python
from automagik_telemetry import AutomagikTelemetry
from automagik_telemetry.client import TelemetryConfig

config = TelemetryConfig(
    project_name="my-app",
    version="1.0.0",
    backend="clickhouse",                           # NEW
    clickhouse_endpoint="http://localhost:8123",    # NEW
    clickhouse_database="telemetry",                # NEW
    clickhouse_table="traces",                      # NEW
)

client = AutomagikTelemetry(config=config)
```

**All your existing code remains the same:**
```python
from automagik_telemetry import MetricType

# These work identically with both backends
client.track_event("user.login", {"user_id": "123"})
client.track_metric("api.requests", value=1, metric_type=MetricType.COUNTER)
client.track_error(exception)
client.flush()
```

#### TypeScript Migration

**Before (OTLP):**
```typescript
import { AutomagikTelemetry } from '@automagik/telemetry';

const client = new AutomagikTelemetry({
    projectName: 'my-app',
    version: '1.0.0',
    endpoint: 'https://telemetry.namastex.ai/v1/traces'  // OTLP endpoint
});
```

**After (ClickHouse):**
```typescript
import { AutomagikTelemetry, TelemetryConfig } from '@automagik/telemetry';

const config: TelemetryConfig = {
    projectName: 'my-app',
    version: '1.0.0',
    backend: 'clickhouse',                          // NEW
    clickhouseEndpoint: 'http://localhost:8123',    // NEW
    clickhouseDatabase: 'telemetry',                // NEW
    clickhouseTable: 'traces',                      // NEW
};

const client = new AutomagikTelemetry(config);
```

**All your existing code remains the same:**
```typescript
import { MetricType } from '@automagik/telemetry';

// These work identically with both backends
client.trackEvent('user.login', { userId: '123' });
client.trackMetric('api.requests', 1, MetricType.COUNTER);
client.trackError(error);
await client.flush();
```

### Backward Compatibility Notes

**Fully backward compatible!**
- Default backend is still OTLP (no breaking changes)
- Existing OTLP configurations continue to work
- You can switch backends per environment
- All SDK methods work identically

**Environment-based switching:**
```bash
# Development: Use ClickHouse
export AUTOMAGIK_TELEMETRY_BACKEND=clickhouse
export AUTOMAGIK_TELEMETRY_CLICKHOUSE_ENDPOINT=http://localhost:8123

# Production: Use OTLP (or just unset the above)
export AUTOMAGIK_TELEMETRY_BACKEND=otlp
export AUTOMAGIK_TELEMETRY_ENDPOINT=https://telemetry.namastex.ai/v1/traces
```

### Testing Your Migration

**1. Unit Tests - No Changes Needed**
```python
# Your existing tests work unchanged
def test_tracking_event():
    client = AutomagikTelemetry(
        project_name="test-app",
        version="1.0.0",
        disabled=True  # Disable for unit tests
    )
    client.track_event("test.event", {"foo": "bar"})
    # Assertions...
```

**2. Integration Tests - Update Endpoint**
```python
# Python integration test
def test_clickhouse_integration():
    config = TelemetryConfig(
        project_name="test-app",
        version="1.0.0",
        backend="clickhouse",
        clickhouse_endpoint="http://localhost:8123",
        batch_size=1,  # Immediate flush for testing
    )
    client = AutomagikTelemetry(config=config)

    # Track event
    client.track_event("test.migration", {"test": "value"})
    client.flush()

    # Verify in ClickHouse
    time.sleep(1)
    result = query_clickhouse(
        "SELECT count() FROM traces WHERE span_name = 'test.migration'"
    )
    assert result[0]['count'] > 0
```

**3. Manual Testing**
```bash
# Start ClickHouse
cd infra && make start

# Run your app with ClickHouse backend
export AUTOMAGIK_TELEMETRY_BACKEND=clickhouse
python your_app.py

# Verify data arrival
clickhouse-client --query="
SELECT count() as events FROM telemetry.traces
WHERE project_name = 'your-app'
AND timestamp > now() - INTERVAL 1 MINUTE
"
```

---

## 5. Advanced Usage

### Custom ClickHouse Endpoints

#### Using Custom Ports

```python
# Python
config = TelemetryConfig(
    backend="clickhouse",
    clickhouse_endpoint="http://clickhouse.internal:9123",  # Custom port
)
```

#### Using HTTPS

```python
# Python - Secure connection
config = TelemetryConfig(
    backend="clickhouse",
    clickhouse_endpoint="https://clickhouse.example.com:8443",
    clickhouse_username="app_user",
    clickhouse_password=os.getenv("CLICKHOUSE_PASSWORD"),
)
```

#### Using ClickHouse Cloud

```python
# Python - ClickHouse Cloud
config = TelemetryConfig(
    backend="clickhouse",
    clickhouse_endpoint="https://abc123.clickhouse.cloud:8443",
    clickhouse_username="default",
    clickhouse_password=os.getenv("CLICKHOUSE_CLOUD_PASSWORD"),
    clickhouse_database="default",
    clickhouse_table="traces",
)
```

### Authentication Setup

#### Basic Authentication

```python
# Python
config = TelemetryConfig(
    backend="clickhouse",
    clickhouse_endpoint="http://localhost:8123",
    clickhouse_username="telemetry_writer",
    clickhouse_password="secure_password_123",
)
```

#### Using Environment Variables for Secrets

```python
# Python - Secure credential management
import os

config = TelemetryConfig(
    backend="clickhouse",
    clickhouse_username=os.getenv("CLICKHOUSE_USER", "default"),
    clickhouse_password=os.getenv("CLICKHOUSE_PASSWORD", ""),
)
```

#### Creating Dedicated ClickHouse User

```sql
-- Connect as admin
clickhouse-client --host localhost --port 9000 --user default

-- Create dedicated user for telemetry
CREATE USER telemetry_writer IDENTIFIED WITH plaintext_password BY 'secure_password';

-- Grant insert permissions only
GRANT INSERT ON telemetry.traces TO telemetry_writer;

-- Verify
SHOW GRANTS FOR telemetry_writer;
```

### Batch Size Tuning

Choose batch size based on your use case:

#### Real-Time Visibility (Small Batches)

```python
# Python - Events appear immediately
config = TelemetryConfig(
    backend="clickhouse",
    batch_size=1,  # Send every event immediately
)

# Good for: Development, debugging, low-volume apps
# Trade-off: Higher network overhead
```

#### Balanced (Default)

```python
# Python - Good balance
config = TelemetryConfig(
    backend="clickhouse",
    batch_size=100,  # Default
)

# Good for: Most production applications
# Trade-off: ~100 events or 10 seconds delay
```

#### High Throughput (Large Batches)

```python
# Python - Optimize for throughput
config = TelemetryConfig(
    backend="clickhouse",
    batch_size=1000,  # Large batches
)

# Good for: High-volume applications, batch processing
# Trade-off: Longer delay before data visible
```

### Compression Configuration

#### When to Enable Compression

```python
# Python - Enable for large payloads
config = TelemetryConfig(
    backend="clickhouse",
    compression_enabled=True,  # Default
    batch_size=500,  # Larger batches compress better
)

# Benefits:
# - ~70-90% bandwidth reduction
# - Better for slow networks
# - More efficient for large attribute maps

# When to use:
# - Production deployments
# - Metered/slow network connections
# - Large batch sizes (100+)
```

#### When to Disable Compression

```python
# Python - Disable for low latency
config = TelemetryConfig(
    backend="clickhouse",
    compression_enabled=False,
    batch_size=10,  # Small batches
)

# Benefits:
# - Lower CPU usage
# - Slightly lower latency
# - Simpler debugging (readable HTTP payloads)

# When to use:
# - Development/debugging
# - Low-volume applications
# - Fast local networks
# - CPU-constrained environments
```

### Performance Optimization

#### Optimize for Latency

```python
# Python - Minimize time to ClickHouse
config = TelemetryConfig(
    backend="clickhouse",
    batch_size=10,              # Small batches
    compression_enabled=False,   # Skip compression
    timeout=2,                  # Fail fast
    max_retries=1,              # Don't retry much
)
```

#### Optimize for Throughput

```python
# Python - Maximize events per second
config = TelemetryConfig(
    backend="clickhouse",
    batch_size=1000,            # Large batches
    compression_enabled=True,    # Compress for bandwidth
    timeout=10,                 # Allow time for large batches
    max_retries=3,              # Retry failed batches
)
```

#### Optimize for Cost (Network)

```python
# Python - Minimize network usage
config = TelemetryConfig(
    backend="clickhouse",
    batch_size=500,             # Balance batch size
    compression_enabled=True,    # Always compress
    timeout=5,
)

# Can reduce network usage by 10-20x with compression + batching
```

### Custom Retry Logic

```python
# Python - Configure retry behavior
config = TelemetryConfig(
    backend="clickhouse",
    max_retries=5,              # Retry up to 5 times
    timeout=10,                 # Allow longer for retries
)

# Retry schedule (exponential backoff):
# Attempt 1: Immediate
# Attempt 2: Wait 1 second
# Attempt 3: Wait 2 seconds
# Attempt 4: Wait 4 seconds
# Attempt 5: Wait 8 seconds
```

### Direct Backend Usage

For advanced use cases, you can use the backend directly:

```python
# Python - Direct backend usage
from automagik_telemetry.backends.clickhouse import ClickHouseBackend

backend = ClickHouseBackend(
    endpoint="http://localhost:8123",
    database="telemetry",
    table="traces",
)

# Send OTLP-formatted span
otlp_span = {
    "traceId": "abc123",
    "spanId": "def456",
    "name": "my.custom.span",
    "startTimeUnixNano": 1729627200000000000,
    "endTimeUnixNano": 1729627200145000000,
    "attributes": [
        {"key": "custom.field", "value": {"stringValue": "custom value"}}
    ],
}

backend.send_trace(otlp_span)
backend.flush()
```

```typescript
// TypeScript - Direct backend usage
import { ClickHouseBackend } from '@automagik/telemetry';

const backend = new ClickHouseBackend({
    endpoint: 'http://localhost:8123',
    database: 'telemetry',
    table: 'traces',
});

// Send OTLP-formatted span
const otlpSpan = {
    traceId: 'abc123',
    spanId: 'def456',
    name: 'my.custom.span',
    startTimeUnixNano: 1729627200000000000,
    endTimeUnixNano: 1729627200145000000,
    attributes: [
        { key: 'custom.field', value: { stringValue: 'custom value' } }
    ],
};

backend.sendTrace(otlpSpan);
await backend.flush();
```

---

## 6. Troubleshooting

### Common Issues and Solutions

#### Issue: "Connection refused" to ClickHouse

**Symptoms:**
```
URLError: <urlopen error [Errno 111] Connection refused>
```

**Solutions:**

1. **Verify ClickHouse is running:**
```bash
# Check if ClickHouse is running
docker ps | grep clickhouse
# OR
curl http://localhost:8123/ping
```

2. **Check endpoint configuration:**
```python
# Make sure endpoint is correct
config = TelemetryConfig(
    clickhouse_endpoint="http://localhost:8123",  # Not https, not port 9000
)
```

3. **Check firewall/network:**
```bash
# Test connectivity
curl http://localhost:8123
# Should return: Ok.
```

#### Issue: Authentication Failed

**Symptoms:**
```
HTTP 403: Password verification failed
```

**Solutions:**

1. **Verify credentials:**
```bash
# Test credentials directly
curl -u username:password http://localhost:8123/?query=SELECT%201
```

2. **Check user exists:**
```sql
-- In clickhouse-client
SHOW USERS;
```

3. **Update configuration:**
```python
config = TelemetryConfig(
    clickhouse_username="telemetry",
    clickhouse_password="telemetry_password",  # Must match ClickHouse user
)
```

#### Issue: Table Does Not Exist

**Symptoms:**
```
HTTP 404: Table telemetry.traces doesn't exist
```

**Solutions:**

1. **Verify table exists:**
```bash
clickhouse-client --query="SHOW TABLES FROM telemetry"
```

2. **Create table:**
```bash
# Use our init script
cd infra
docker exec -i automagik-clickhouse clickhouse-client < clickhouse/init-db.sql
```

3. **Check database/table names:**
```python
config = TelemetryConfig(
    clickhouse_database="telemetry",  # Must match existing database
    clickhouse_table="traces",        # Must match existing table
)
```

#### Issue: No Data Appearing in ClickHouse

**Symptoms:**
- SDK sends without errors
- ClickHouse is running
- But `SELECT count() FROM traces` returns 0

**Solutions:**

1. **Force flush:**
```python
# Make sure to flush batches
client.track_event("test", {})
client.flush()  # Important!
```

2. **Reduce batch size for testing:**
```python
config = TelemetryConfig(
    backend="clickhouse",
    batch_size=1,  # Immediate send
)
```

3. **Enable verbose logging:**
```python
import logging
logging.basicConfig(level=logging.DEBUG)

# Or via environment
export AUTOMAGIK_TELEMETRY_VERBOSE=true
```

4. **Check ClickHouse logs:**
```bash
docker logs automagik-clickhouse | tail -50
```

### Debug Mode

Enable detailed logging to diagnose issues:

```python
# Python
import logging

# Enable DEBUG logging for the SDK
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

# Now you'll see all HTTP requests and responses
client = AutomagikTelemetry(config=config)
client.track_event("debug.test", {})
client.flush()
```

**Expected debug output:**
```
2025-10-22 10:30:15 - automagik_telemetry.backends.clickhouse - DEBUG - Inserted 1 rows to ClickHouse successfully
2025-10-22 10:30:15 - automagik_telemetry.backends.clickhouse - DEBUG - POST http://localhost:8123/?query=INSERT%20INTO%20telemetry.traces%20FORMAT%20JSONEachRow
2025-10-22 10:30:15 - automagik_telemetry.backends.clickhouse - DEBUG - Response: 200 OK
```

### Checking ClickHouse Connectivity

**Test 1: Basic connectivity**
```bash
curl http://localhost:8123/ping
# Expected: Ok.
```

**Test 2: Query test**
```bash
curl "http://localhost:8123/?query=SELECT%201"
# Expected: 1
```

**Test 3: Authenticated query**
```bash
curl -u telemetry:telemetry_password \
  "http://localhost:8123/?query=SELECT%20version()"
# Expected: 24.8.1.1 (or your ClickHouse version)
```

**Test 4: Database access**
```bash
curl -u telemetry:telemetry_password \
  "http://localhost:8123/?query=SELECT%20count()%20FROM%20telemetry.traces"
# Expected: <number>
```

### Viewing Data in ClickHouse

**Method 1: CLI**
```bash
clickhouse-client --host localhost --port 9000 --query="
SELECT
    timestamp,
    project_name,
    span_name,
    status_code,
    attributes
FROM telemetry.traces
ORDER BY timestamp DESC
LIMIT 10
FORMAT PrettyCompact
"
```

**Method 2: HTTP API**
```bash
curl "http://localhost:8123/?query=SELECT%20*%20FROM%20telemetry.traces%20LIMIT%2010%20FORMAT%20JSONEachRow"
```

**Method 3: Grafana Explore**
```bash
# Open Grafana
open http://localhost:3000

# Navigate to Explore (compass icon)
# Select ClickHouse datasource
# Enter query:
SELECT * FROM traces
WHERE project_name = 'my-app'
ORDER BY timestamp DESC
LIMIT 100
```

### Performance Issues

**Issue: High latency on flush**

**Solutions:**
1. Increase batch size to reduce HTTP overhead
2. Enable compression for large batches
3. Check network connectivity to ClickHouse
4. Consider async insertion mode (ClickHouse feature)

**Issue: High memory usage**

**Solutions:**
1. Reduce batch size
2. Ensure flush() is called regularly
3. Check for long-running processes without cleanup

**Issue: Events being dropped**

**Solutions:**
1. Check ClickHouse disk space
2. Verify write permissions
3. Check for schema mismatches
4. Review ClickHouse logs for errors

---

## 7. Production Deployment

### Best Practices

#### 1. Use HTTPS with Valid Certificates

```python
# Python - Production config
config = TelemetryConfig(
    backend="clickhouse",
    clickhouse_endpoint="https://clickhouse.internal:8443",  # HTTPS
    clickhouse_username=os.getenv("CLICKHOUSE_USER"),
    clickhouse_password=os.getenv("CLICKHOUSE_PASSWORD"),
)
```

**Why:** Encrypts data in transit, prevents MITM attacks.

#### 2. Separate User with Limited Permissions

```sql
-- Create write-only user
CREATE USER telemetry_writer IDENTIFIED WITH plaintext_password BY 'secure_password';

-- Grant minimal permissions
GRANT INSERT ON telemetry.traces TO telemetry_writer;

-- No SELECT, no DELETE, no ALTER
```

**Why:** Limits damage from compromised credentials.

#### 3. Use Secrets Management

```python
# Python - Use secrets manager (AWS Secrets Manager, HashiCorp Vault, etc.)
import boto3

def get_clickhouse_password():
    client = boto3.client('secretsmanager')
    response = client.get_secret_value(SecretId='prod/clickhouse/password')
    return response['SecretString']

config = TelemetryConfig(
    clickhouse_password=get_clickhouse_password(),
)
```

**Why:** Never hardcode passwords, rotate easily.

#### 4. Configure Appropriate Batch Sizes

```python
# Production - Balance latency and throughput
config = TelemetryConfig(
    batch_size=500,  # 500 events or ~10 seconds
)
```

**Why:** Too small = network overhead, too large = memory issues.

#### 5. Enable Compression

```python
config = TelemetryConfig(
    compression_enabled=True,  # Always enable in production
)
```

**Why:** Reduces bandwidth costs by 70-90%.

#### 6. Set Reasonable Timeouts

```python
config = TelemetryConfig(
    timeout=5,        # Fail fast
    max_retries=3,    # Limited retries
)
```

**Why:** Prevents hanging requests from blocking application.

### Security Considerations

#### Network Security

**Firewall Rules:**
```bash
# Only allow application servers to access ClickHouse
# ClickHouse server (8123, 9000)
ufw allow from 10.0.1.0/24 to any port 8123 proto tcp
ufw allow from 10.0.1.0/24 to any port 9000 proto tcp
```

**TLS Configuration:**
```xml
<!-- ClickHouse config.xml -->
<clickhouse>
    <openSSL>
        <server>
            <certificateFile>/etc/clickhouse-server/server.crt</certificateFile>
            <privateKeyFile>/etc/clickhouse-server/server.key</privateKeyFile>
            <dhParamsFile>/etc/clickhouse-server/dhparam.pem</dhParamsFile>
            <verificationMode>strict</verificationMode>
        </server>
    </openSSL>
</clickhouse>
```

#### Authentication & Authorization

**Use dedicated users per application:**
```sql
-- App 1
CREATE USER app1_telemetry IDENTIFIED WITH plaintext_password BY 'app1_password';
GRANT INSERT ON telemetry.traces TO app1_telemetry;

-- App 2
CREATE USER app2_telemetry IDENTIFIED WITH plaintext_password BY 'app2_password';
GRANT INSERT ON telemetry.traces TO app2_telemetry;
```

**Rotate passwords regularly:**
```sql
ALTER USER telemetry_writer IDENTIFIED WITH plaintext_password BY 'new_password';
```

#### Data Security

**Enable TTL for automatic cleanup:**
```sql
ALTER TABLE telemetry.traces
MODIFY TTL timestamp + INTERVAL 90 DAY;
```

**Encrypt data at rest (ClickHouse feature):**
```xml
<clickhouse>
    <encryption_codecs>
        <aes_128_gcm_siv>
            <key>your-encryption-key</key>
        </aes_128_gcm_siv>
    </encryption_codecs>
</clickhouse>
```

### Monitoring Recommendations

#### Application-Level Monitoring

**Track telemetry health:**
```python
# Python - Monitor telemetry itself
import time
import logging

class TelemetryMonitor:
    def __init__(self, client):
        self.client = client
        self.events_sent = 0
        self.errors = 0
        self.last_flush = time.time()

    def track_with_monitoring(self, event_name, attributes):
        try:
            self.client.track_event(event_name, attributes)
            self.events_sent += 1
        except Exception as e:
            self.errors += 1
            logging.error(f"Telemetry error: {e}")

    def health_check(self):
        return {
            "events_sent": self.events_sent,
            "errors": self.errors,
            "error_rate": self.errors / max(self.events_sent, 1),
            "time_since_flush": time.time() - self.last_flush,
        }
```

#### ClickHouse Monitoring

**Key metrics to monitor:**
```sql
-- Query performance
SELECT
    query,
    query_duration_ms,
    read_rows,
    written_rows
FROM system.query_log
WHERE type = 'QueryFinish'
AND query_duration_ms > 1000
ORDER BY query_duration_ms DESC
LIMIT 10;

-- Insert performance
SELECT
    event_time,
    CurrentMetric_BackgroundPoolTask,
    CurrentMetric_InsertQuery
FROM system.metrics
WHERE event_time > now() - INTERVAL 1 HOUR;

-- Disk usage
SELECT
    name,
    path,
    formatReadableSize(free_space) as free,
    formatReadableSize(total_space) as total
FROM system.disks;

-- Table size
SELECT
    table,
    formatReadableSize(sum(bytes)) as size,
    sum(rows) as rows
FROM system.parts
WHERE database = 'telemetry'
GROUP BY table;
```

#### Alerts to Configure

**Application Alerts:**
- Telemetry error rate > 5%
- Time since last successful flush > 60 seconds
- Batch queue size growing unbounded

**ClickHouse Alerts:**
- Disk usage > 80%
- Insert query latency > 5 seconds
- Failed inserts > 1% of total
- Replication lag > 60 seconds (if using replication)

### Scaling Guidelines

#### Vertical Scaling (Single Server)

**When to scale:**
- CPU usage consistently > 70%
- Memory usage > 80%
- Disk I/O saturation
- Insert latency increasing

**How to scale:**
```yaml
# Increase ClickHouse resources
services:
  clickhouse:
    deploy:
      resources:
        limits:
          cpus: '8'
          memory: 32G
        reservations:
          cpus: '4'
          memory: 16G
```

#### Horizontal Scaling (Distributed)

**Setup ClickHouse cluster:**
```xml
<!-- config.xml -->
<clickhouse>
    <remote_servers>
        <telemetry_cluster>
            <shard>
                <replica>
                    <host>clickhouse1</host>
                    <port>9000</port>
                </replica>
                <replica>
                    <host>clickhouse2</host>
                    <port>9000</port>
                </replica>
            </shard>
        </telemetry_cluster>
    </remote_servers>
</clickhouse>
```

**Use distributed table:**
```sql
-- Create distributed table
CREATE TABLE telemetry.traces_distributed AS telemetry.traces
ENGINE = Distributed(telemetry_cluster, telemetry, traces, rand());

-- Update SDK config to use distributed table
-- (table name changes to traces_distributed)
```

#### Application-Side Scaling

**Load balance across multiple ClickHouse servers:**
```python
# Python - Round-robin across servers
import random

CLICKHOUSE_ENDPOINTS = [
    "http://clickhouse1:8123",
    "http://clickhouse2:8123",
    "http://clickhouse3:8123",
]

config = TelemetryConfig(
    backend="clickhouse",
    clickhouse_endpoint=random.choice(CLICKHOUSE_ENDPOINTS),
)
```

#### Partitioning Strategy

```sql
-- Partition by month for easy archival
CREATE TABLE telemetry.traces (
    ...
) ENGINE = MergeTree()
PARTITION BY toYYYYMM(timestamp)
ORDER BY (project_name, timestamp, trace_id)
TTL timestamp + INTERVAL 90 DAY;

-- Old partitions automatically dropped by TTL
-- Or manually: ALTER TABLE traces DROP PARTITION '202401';
```

#### Performance Tuning

**ClickHouse server settings:**
```xml
<!-- config.xml -->
<clickhouse>
    <!-- Increase max concurrent queries -->
    <max_concurrent_queries>200</max_concurrent_queries>

    <!-- Increase insert buffer -->
    <max_insert_block_size>1048576</max_insert_block_size>

    <!-- Enable async inserts -->
    <async_insert>1</async_insert>
    <wait_for_async_insert>0</wait_for_async_insert>
</clickhouse>
```

**SDK configuration:**
```python
# Tune for high throughput
config = TelemetryConfig(
    backend="clickhouse",
    batch_size=1000,           # Large batches
    compression_enabled=True,   # Compress
    timeout=10,                # Allow time
)
```

---

## Conclusion

The ClickHouse backend provides a simple, high-performance alternative to OTLP for self-hosted telemetry. With minimal configuration changes, you can:

- Simplify your infrastructure (no collector needed)
- Improve performance (direct insertion)
- Maintain full control (custom schema)
- Scale effectively (leverage ClickHouse's capabilities)

**Next Steps:**

1. Start with local development using Docker Compose
2. Test thoroughly in staging environment
3. Deploy to production with proper security
4. Monitor and optimize based on your workload

**Resources:**

- [ClickHouse Backend Design Document](../infra/CLICKHOUSE_BACKEND_DESIGN.md)
- [Infrastructure Setup Guide](../infra/README.md)
- [SDK Documentation](../README.md)
- [ClickHouse Documentation](https://clickhouse.com/docs)

**Need Help?**

- GitHub Issues: https://github.com/namastexlabs/automagik-telemetry/issues
- Discord: https://discord.gg/xcW8c7fF3R
- Email: support@namastex.ai
